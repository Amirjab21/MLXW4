{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amirjabarivasal/Documents/MLX/week4/week4/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageOps\n",
    "dataset = load_dataset(\"nlphuji/flickr30k\")\n",
    "dataset = dataset['test'].select(range(100))\n",
    "\n",
    "def make_square(image):\n",
    "    # Make image square\n",
    "    size = max(image.size)\n",
    "    new_image = ImageOps.pad(image, (size, size), color='white')\n",
    "    new_image = new_image.resize((256, 256))\n",
    "    # fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    # ax[0].imshow(image, cmap='gray')\n",
    "    # ax[1].imshow(new_image, cmap='gray')\n",
    "    # plt.show()\n",
    "    return new_image\n",
    "\n",
    "def transform_images(examples):\n",
    "    image = examples['image']\n",
    "    processed_image = np.array(make_square(image))\n",
    "    return {\n",
    "        'image': examples['image'],  # Keep original image\n",
    "        'image_processed': processed_image,  # Add processed image\n",
    "        # 'caption': examples['caption']  # Keep other fields\n",
    "    }\n",
    "\n",
    "transformed_images = dataset.map(transform_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP = transformers.CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = transformers.CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "# Access vocabulary\n",
    "vocab = tokenizer.get_vocab()  # Returns dict of {token: index}\n",
    "vocab_size = tokenizer.vocab_size  # Get total vocabulary size\n",
    "\n",
    "text_model = CLIP.text_model\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dataset = self.dataset[idx]\n",
    "        image = dataset['image_processed'] #height, width, channels\n",
    "        captions = dataset['caption']\n",
    "        \n",
    "        patches_array = split_image_to_patches(image, image.shape[0], image.shape[1], 32, 3)\n",
    "        patches_tensor = [torch.tensor(patch.flatten(), dtype=torch.float32) for patch in patches_array] \n",
    "        image_tensor = torch.stack(patches_tensor)\n",
    "\n",
    "\n",
    "        \n",
    "        random_caption_idx = torch.randint(0, len(captions), (1,)).item()\n",
    "        selected_caption = captions[random_caption_idx]\n",
    "        tokenized_caption = tokenizer(selected_caption, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            'image': image_tensor,\n",
    "            'caption': tokenized_caption['input_ids'],\n",
    "        }\n",
    "    \n",
    "reverse_vocab = {idx: token for token, idx in vocab.items()}  # {index: token}\n",
    "\n",
    "datasetclass = Dataset(transformed_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import copy\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, text_encoder, image_encoder, decoder, tgt_vocab_size):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.fc = nn.Linear(tgt_vocab_size, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, image, caption):\n",
    "        print(caption.shape, \"caption.shape\")\n",
    "        # text_encoder_output = self.text_encoder.forward(caption)\n",
    "        # print(text_encoder_output)\n",
    "        # print(text_encoder_output.shape, \"text_encoder_output.shape\")\n",
    "        image_encoder_output = self.image_encoder.forward(image)\n",
    "        # print(image_encoder_output.last_hidden_state, \"image_encoder_output.shape\")\n",
    "        # print(text_encoder_output.shape, image_encoder_output.shape)\n",
    "        dec_output = self.decoder.forward(caption, image_encoder_output.last_hidden_state)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, input_dim, tgt_vocab_size, intermediate_attn_dim, n_loops, feed_forward, self_attn_layer, cross_attn_layer):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn_layer = self_attn_layer\n",
    "        self.cross_attn_layer = cross_attn_layer\n",
    "        self.FF_layer = feed_forward\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.input_dim = input_dim\n",
    "        self.n_loops = n_loops\n",
    "\n",
    "        self.projectbacktovocab = torch.nn.Linear(intermediate_attn_dim, tgt_vocab_size)\n",
    "\n",
    "        self.norm1 = torch.nn.LayerNorm(input_dim)\n",
    "        self.norm2 = torch.nn.LayerNorm(input_dim)\n",
    "        self.norm3 = torch.nn.LayerNorm(input_dim)\n",
    "\n",
    "    def forward(self, x, encoder_output, mask):\n",
    "        embedding = x\n",
    "        attn, prob = self.self_attn_layer.forward(embedding, embedding, embedding, mask)\n",
    "        print(attn.shape, \"attn.shape\")\n",
    "        print(embedding.shape, \"embedding.shape\")\n",
    "        x = self.norm1(attn + embedding)\n",
    "        attn, prob = self.cross_attn_layer.forward(query_input=x, key_input=encoder_output, value_input=encoder_output)\n",
    "        print(attn.shape, \"cross attn.shape\")\n",
    "        # attn = self.projectbacktovocab(attn)\n",
    "        x = self.norm2(x + attn)\n",
    "\n",
    "        ff_output = self.FF_layer(x)\n",
    "        x = self.norm3(x + ff_output)\n",
    "        # x = self.projectbacktovocab(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "     def __init__(self, tgt_vocab_size, pad_token, embedding_layer, layer, n_loops):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding_layer = embedding_layer #convert token IDs to embeddings\n",
    "        self.pad_token = pad_token\n",
    "        self.norm1 = torch.nn.LayerNorm(tgt_vocab_size)\n",
    "        self.layers = clones(layer, n_loops)\n",
    "\n",
    "        self.projectbacktovocab = torch.nn.Linear(512, tgt_vocab_size)\n",
    "\n",
    "\n",
    "     def forward(self, x, encoder_output):\n",
    "        # mask = self.generate_padding_mask(x)\n",
    "        mask = True\n",
    "        print(x.shape, \"x.shape pre embedding\")\n",
    "        x = self.embedding_layer.forward(x)\n",
    "        print(x.shape, \"x.shape\")\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, mask)\n",
    "        x = self.projectbacktovocab(x)\n",
    "        x = self.norm1(x)\n",
    "        return x\n",
    "     \n",
    "   \n",
    "\n",
    "class Attention_Layer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(Attention_Layer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.W_q = torch.nn.Linear(d_model, d_model)\n",
    "        self.W_k = torch.nn.Linear(d_model, d_model)\n",
    "        self.W_v = torch.nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, query_input, key_input, value_input, mask=None):\n",
    "        dim_k = self.d_model // self.num_heads\n",
    "        query = self.W_q(query_input)\n",
    "        key = self.W_k(key_input)\n",
    "        value = self.W_v(value_input)\n",
    "        \n",
    "\n",
    "        \n",
    "        query_key = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(dim_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            nopeak_mask = (1 - torch.triu(torch.ones(query_key.size(-2), query_key.size(-1)), diagonal=1)).bool()\n",
    "            query_key = query_key.masked_fill(~nopeak_mask, float('-inf'))\n",
    "            # print(mask.shape, 'mask.shape', query_key.shape)\n",
    "            # query_key = query_key.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "        prob = query_key.softmax(dim=-1)\n",
    "        weighted_attention = torch.matmul(prob, value)\n",
    "        return weighted_attention, prob\n",
    "    \n",
    "class Cross_Attention_Layer(nn.Module):\n",
    "    def __init__(self, encoder_output_dim, decoder_dim, d_model, num_heads):\n",
    "        super(Cross_Attention_Layer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.W_q = torch.nn.Linear(decoder_dim, d_model)\n",
    "\n",
    "        self.W_k = torch.nn.Linear(encoder_output_dim, d_model)\n",
    "        self.W_v = torch.nn.Linear(encoder_output_dim, d_model)\n",
    "    \n",
    "    def forward(self, query_input, key_input, value_input, mask=None):\n",
    "        dim_k = self.d_model // self.num_heads\n",
    "        query = self.W_q(query_input)\n",
    "        key = self.W_k(key_input)\n",
    "        value = self.W_v(value_input)\n",
    "        \n",
    "        query_key = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(dim_k)\n",
    "        prob = query_key.softmax(dim=-1)\n",
    "        weighted_attention = torch.matmul(prob, value)\n",
    "        return weighted_attention, prob\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "text_dimension_embedding = 512\n",
    "image_encoder_output_dim = 768\n",
    "\n",
    "self_attn_layer = Attention_Layer(d_model=d_model, num_heads=1)\n",
    "cross_attn_layer = Cross_Attention_Layer(encoder_output_dim=image_encoder_output_dim, decoder_dim=text_dimension_embedding, d_model=d_model, num_heads=1)\n",
    "\n",
    "feed_forward = nn.Sequential(nn.Linear(d_model, 2048), nn.ReLU(), nn.Linear(2048, d_model))\n",
    "\n",
    "text_model = CLIP.text_model\n",
    "text_embedder = text_model.embeddings\n",
    "\n",
    "decoder_layer = DecoderLayer(input_dim=text_dimension_embedding, tgt_vocab_size=vocab_size, intermediate_attn_dim=d_model, n_loops=6, feed_forward=feed_forward, self_attn_layer=self_attn_layer, cross_attn_layer=cross_attn_layer)\n",
    "\n",
    "\n",
    "\n",
    "decoder = Decoder(vocab_size, pad_token=\"pad\", embedding_layer=text_embedder, layer=decoder_layer, n_loops=6)\n",
    "\n",
    "\n",
    "transformer = Transformer(d_model=d_model, text_encoder=text_embedder, image_encoder=CLIP.vision_model, decoder=decoder, tgt_vocab_size=vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformed_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransformed_images\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformed_images' is not defined"
     ]
    }
   ],
   "source": [
    "transformed_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 50, 768]) vision_output.shape\n",
      "torch.Size([1, 77, 512])\n"
     ]
    }
   ],
   "source": [
    "image_processed = processor(images=np.array(transformed_images[0]['image_processed']), return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# token_ids = token_ids['input_ids']\n",
    "print(image_processed['pixel_values'].shape)\n",
    "visionmodel = CLIP.vision_model\n",
    "visionmodel.eval()\n",
    "vision_output = visionmodel(image_processed['pixel_values'])\n",
    "print(vision_output.last_hidden_state.shape, \"vision_output.shape\")\n",
    "\n",
    "\n",
    "text = \"hello i am dog\"\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", max_length=77, truncation=True)\n",
    "textmodel = CLIP.text_model\n",
    "# textmodel.eval()\n",
    "\n",
    "text_encoder_output = textmodel(tokenized_text['input_ids'])\n",
    "print(text_encoder_output.last_hidden_state.shape)\n",
    "last_hidden_state_text = text_encoder_output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[49406,  3306,   328,   687,  1929, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407]])\n",
      "tensor(False)\n",
      "tensor(True)\n",
      "torch.Size([1, 77])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text['input_ids'])\n",
    "captions = tokenized_text['input_ids']\n",
    "print((captions != tokenizer.pad_token_id).all())\n",
    "print((captions != tokenizer.pad_token_id).any() != (captions != tokenizer.pad_token_id).all())\n",
    "if (captions != tokenizer.pad_token_id).any() != (captions != tokenizer.pad_token_id).all():\n",
    "    attention_mask = (captions != tokenizer.pad_token_id)\n",
    "\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "def extend_padding_mask(padding_mask, batch_size, device):\n",
    "    \"\"\"Extend padding mask to include image token.\"\"\"\n",
    "    if padding_mask is None:\n",
    "        return None\n",
    "    \n",
    "    # Create mask for image token (always valid)\n",
    "    image_mask = torch.ones(padding_mask.shape[0], 1, dtype=torch.bool, device=device)\n",
    "    \n",
    "    # Concatenate with caption padding mask\n",
    "    extended_mask = torch.cat([image_mask, padding_mask], dim=1)\n",
    "    \n",
    "    return extended_mask \n",
    "extended = extend_padding_mask(attention_mask, 5, torch.device(\"cpu\"))\n",
    "print(extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[49406,  3306,   328,   687,  1929, 49407]])\n"
     ]
    }
   ],
   "source": [
    "text = \"hello i am dog\"\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "print(tokenized_text['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49407"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "textmodel = CLIP.text_model\n",
    "# textmodel.eval()\n",
    "\n",
    "text_encoder_output = textmodel(tokenized_text['input_ids'])\n",
    "processed_full = processor(images=[np.array(transformed_images[0]['image_processed'])], text=\"hello i am dog\", return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(CLIP(**processed_full).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
      "         [ 0.3177, -0.4308, -0.8810,  ...,  0.5321, -0.7913, -0.4851],\n",
      "         [ 0.8905, -0.1765,  0.1217,  ...,  0.8202, -0.1785,  0.4732],\n",
      "         [-0.3952,  1.1094,  0.1687,  ...,  0.8740, -0.4523, -0.5020],\n",
      "         [-0.2759,  0.5866,  1.2806,  ..., -0.0064,  0.5442, -1.3609],\n",
      "         [ 0.7603,  0.5137,  0.9196,  ..., -0.8102, -0.2604, -0.0036]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 7.6033e-01,  5.1368e-01,  9.1960e-01,  1.2506e+01, -8.6933e+00,\n",
      "          1.0663e+00,  2.1425e+00,  1.4863e+00, -6.2332e-01,  5.3471e-01,\n",
      "          1.5366e+00, -1.3907e+00,  2.8199e-01,  1.3469e-01,  6.0156e-01,\n",
      "         -1.6914e+00, -1.7101e+00, -5.1569e-01, -1.7452e-01,  2.9796e-01,\n",
      "          8.0737e-01, -3.0274e-01, -5.0148e-01,  2.9735e-01,  1.2268e-01,\n",
      "         -5.9684e-01, -1.2002e+00, -5.7734e-01, -6.6179e-01,  5.5981e-01,\n",
      "          4.0388e-01,  7.9476e-01, -2.2371e+00, -5.2926e-01, -9.6208e-01,\n",
      "         -5.4188e-01, -8.1370e-01,  1.2500e+00,  5.9161e-01,  1.1861e+00,\n",
      "          6.2291e-01, -1.2906e+00, -4.3989e-01, -1.6166e+00,  3.3394e-01,\n",
      "          6.8429e-01,  4.9264e-01, -3.6405e-01,  5.2603e-01,  1.3725e-01,\n",
      "         -2.2766e-01,  8.8136e-01,  1.0736e+00,  1.5551e+00,  8.6274e-01,\n",
      "         -1.1349e-01,  7.4855e-01,  8.0727e-01,  1.0892e+00,  9.7095e-01,\n",
      "         -1.2347e+00,  2.2589e-03,  1.6204e+00,  8.6949e-01, -1.7538e+00,\n",
      "          9.5000e-01, -5.3725e-01, -8.4708e-01,  3.7863e-01,  7.4628e-01,\n",
      "         -6.3721e-01,  7.2505e-01, -9.8242e-01, -1.8658e+00, -7.4639e-01,\n",
      "          9.4835e-01,  2.0177e-01, -5.0149e-01, -4.4878e-01,  1.2106e+00,\n",
      "          1.5845e+00, -2.1263e-01, -3.6469e-01,  5.1434e-01,  5.7931e-02,\n",
      "          4.8559e-01, -1.1554e+00,  1.3692e+00,  1.4879e-01,  7.3545e-01,\n",
      "         -7.8072e-02,  1.8255e+00, -4.7963e-01, -5.0060e-02, -6.6577e-01,\n",
      "          7.8552e-01,  9.1350e-01,  8.0018e-01, -4.3040e-01,  1.0447e+00,\n",
      "          1.1123e+00, -2.8250e-01,  7.8072e-01, -9.9005e-01,  6.1175e-01,\n",
      "         -1.0500e+00,  2.6910e-01,  7.3827e-01,  7.9291e-01, -1.8264e-01,\n",
      "          5.0422e-01,  9.5641e-02, -2.8417e+00,  7.8111e-01, -1.3957e+00,\n",
      "          5.1606e-01,  4.8523e-01, -1.6686e-01,  5.0849e-01,  3.3411e-01,\n",
      "          1.7280e-01,  8.8143e-01, -1.2419e+00,  9.7111e-01,  2.2762e+00,\n",
      "          1.8681e-01,  2.0903e-01,  1.4480e+00, -3.7532e-01, -7.4722e-01,\n",
      "          2.9286e-01,  1.1899e+00,  6.0831e-01,  1.0807e-01,  2.3818e-01,\n",
      "         -8.9213e-01,  1.7024e+00, -1.0553e-01, -5.8010e-02,  2.5474e-01,\n",
      "          7.1886e-01, -9.4507e-01,  2.4556e+00, -5.0432e-01,  3.8135e-01,\n",
      "          6.6584e-01, -6.5314e-01, -4.4722e-01, -6.2425e-01, -7.5465e-01,\n",
      "         -6.3719e-01, -3.1713e-02,  1.5050e+00, -1.0714e-01, -1.7109e+00,\n",
      "          6.3725e-01,  9.0144e-01, -2.7402e-01,  8.9184e-01, -1.2378e+00,\n",
      "         -6.2304e-01,  9.7281e-01, -1.0287e+00,  7.7041e-01, -1.5144e+00,\n",
      "          1.5876e-02,  4.2380e-03, -1.0618e+00,  1.7982e+00, -1.2842e+00,\n",
      "          5.3214e-01, -4.2474e-04,  3.5698e-01,  7.9409e-01, -5.6657e-01,\n",
      "         -3.4861e-02,  3.9212e-01,  8.3211e-01, -2.7471e-01, -2.5607e-01,\n",
      "         -6.7592e-01,  1.3730e+00,  2.3690e-01, -1.0972e-01, -1.2331e+00,\n",
      "         -2.3300e-01,  1.7643e-01, -6.7669e-01,  5.1706e-01,  5.6748e-01,\n",
      "          5.3768e-01, -5.6305e-01,  4.3707e-01,  1.1594e+00,  8.8635e-01,\n",
      "         -6.5479e-01,  2.4198e-01, -1.8689e+00,  2.9299e-03,  3.9763e-01,\n",
      "         -3.0088e-01,  9.0768e-01,  1.3085e+00,  4.0057e+00,  1.2685e+00,\n",
      "         -1.7228e-02, -1.6851e+00,  9.0361e-01, -2.2770e-01,  9.3376e-01,\n",
      "          9.0999e-01, -1.0341e-01, -6.7851e-01, -2.6924e-01,  4.1353e-01,\n",
      "          6.0931e-02, -2.2686e-01, -3.4632e-01, -1.6801e-01, -4.5562e-01,\n",
      "          2.7479e-01,  9.7341e-01, -6.5284e-01,  7.4608e-01,  1.9952e+00,\n",
      "         -3.5428e-01, -6.9423e-02,  3.1027e-01,  2.6080e-01, -2.5801e-01,\n",
      "         -6.9550e-02,  8.3659e-01, -1.3862e+00,  3.5516e-02, -1.2860e-01,\n",
      "         -1.5256e+00,  1.9275e-01,  2.0704e-02,  2.4591e-01,  2.0676e+00,\n",
      "         -6.7605e-02,  7.6535e-01,  1.9766e-01, -1.2469e+00,  1.6062e-01,\n",
      "         -2.1260e+00,  1.5319e+00, -8.0188e-01, -1.5893e+00,  1.5899e+00,\n",
      "          5.9764e-01,  2.4947e-01, -3.4685e-02,  1.9632e+00,  8.5342e-02,\n",
      "         -5.7463e-01,  1.3974e+00, -8.6582e-01,  1.6476e+00, -1.2183e+00,\n",
      "          6.6457e-01, -3.0862e-01,  2.9152e-01,  1.5490e+00, -1.3791e+00,\n",
      "          5.6463e-01,  1.8503e-01,  1.0896e+00,  1.9177e-01,  6.2941e-01,\n",
      "         -2.7914e-01,  7.4289e-01, -1.0183e-01,  6.2575e-01,  2.4645e-01,\n",
      "          1.7234e+00,  1.8593e-01,  2.0370e+00,  1.1592e+00, -8.9644e-01,\n",
      "          3.6203e-01,  4.2003e-01, -1.0013e+00,  1.4852e-01, -2.3000e-01,\n",
      "          1.0863e+00, -1.8693e-01,  5.3068e-01, -3.1121e-01,  1.7427e-01,\n",
      "         -1.2515e+00, -1.0425e+00, -1.2014e+00,  8.9229e-01, -1.6583e+00,\n",
      "          4.2259e-01,  2.0093e-02, -8.5878e-01,  7.2465e-01, -2.9204e-01,\n",
      "          7.8591e-01, -2.9910e-01, -3.8271e-01, -1.5512e+00,  2.0794e-01,\n",
      "         -4.8721e-02, -2.1411e+00, -2.8066e-01,  9.9413e-01,  2.1529e+00,\n",
      "          6.6687e-01,  4.2028e-01,  1.2845e+00,  1.6171e-01, -9.4609e-01,\n",
      "          1.0017e+00, -2.8537e-01,  4.7069e-02, -1.3436e-01, -8.0348e-02,\n",
      "         -9.4545e-02,  4.3836e-01,  4.2331e-01, -4.4113e-01,  1.9650e+00,\n",
      "         -7.4670e-01,  3.8283e-01,  1.6873e+00, -1.5008e-01,  9.5699e-01,\n",
      "         -2.3253e+00,  8.9472e-01, -4.9688e-01,  8.5215e-01, -2.4580e-01,\n",
      "          5.3104e-01, -4.7621e-01, -8.1253e-01, -7.1528e-01, -6.4415e-02,\n",
      "          9.6607e-02,  1.6626e-01,  3.0461e-01,  3.0818e-01,  1.1144e+00,\n",
      "          3.4727e-01, -1.6705e-01,  8.4936e-02,  1.3389e+00,  1.8982e+00,\n",
      "          6.0850e-02, -9.5004e-01, -3.2286e-01, -2.2231e+00, -6.5284e-01,\n",
      "          6.9446e-01, -7.0373e-02, -1.1619e+00, -1.0308e+00,  4.6187e-02,\n",
      "          4.3744e-01, -1.4166e-01, -1.9072e+00,  4.6423e-01,  5.7700e-01,\n",
      "          8.8683e-01, -1.2426e+00, -1.5239e+00,  3.1143e-01, -2.4164e-01,\n",
      "          7.7213e-01,  6.6566e-01,  2.2335e-01, -1.4797e+00, -1.6325e-01,\n",
      "         -1.2571e+00,  6.1751e-01,  5.1384e-01,  9.5116e-01,  8.4049e-01,\n",
      "         -1.3496e+00,  1.6537e-01,  1.7732e-01,  1.1847e+00,  2.4774e-01,\n",
      "          6.6407e-01, -1.2185e-01, -9.8145e-01,  5.2350e-01, -4.7782e-01,\n",
      "          1.1385e+00, -3.6575e-01,  1.3799e-01,  2.7936e-01,  1.2427e+00,\n",
      "          3.5323e-01, -7.8529e-01, -6.1339e-01,  6.9312e-01,  4.5632e-01,\n",
      "         -3.0872e-02, -6.1471e-01,  5.8685e-01,  2.3718e-01, -5.5940e-01,\n",
      "          4.0782e-02, -3.4439e-01, -1.1662e-01,  8.2881e-03,  3.4437e+00,\n",
      "         -1.6676e-01,  4.2021e-01, -1.8577e-03,  1.8119e+00,  5.7378e-01,\n",
      "          1.4041e+00, -1.6620e-02,  1.2996e+00,  1.6127e+00, -4.1219e-01,\n",
      "          4.4993e-01, -8.8199e-01,  4.9804e-01,  4.8531e-01, -9.3937e-01,\n",
      "          1.0515e+00, -2.2008e+00,  7.4490e-01,  8.3572e-01, -1.3427e-01,\n",
      "         -1.4676e+00, -1.1015e+00,  1.9373e-01,  4.9911e-01,  8.0540e-01,\n",
      "         -2.3134e-01,  6.0379e-01,  3.1762e-01, -1.1941e+00,  9.4652e-01,\n",
      "         -7.7357e-01,  9.7874e-01,  1.2776e-01,  4.6861e-01,  1.7057e+00,\n",
      "          7.8731e-01,  1.4463e+00, -1.0368e+00,  1.2260e-01, -1.1294e+00,\n",
      "          9.2161e-01,  4.5293e-01,  2.8637e-01, -9.2891e-01, -1.0479e+00,\n",
      "         -7.5994e-01,  7.1970e-01,  2.1241e-01, -1.5480e-01, -4.9553e-01,\n",
      "         -9.3477e-02, -3.8638e-01, -1.7235e+00,  5.9843e-01,  5.9693e-02,\n",
      "         -6.6008e-01, -2.7217e-01,  3.0032e-01,  1.1265e-01,  9.8694e-01,\n",
      "          5.7834e-01, -6.2957e-01,  3.6404e-01,  2.6526e-01, -9.2468e-02,\n",
      "          1.7419e+00, -7.1511e-01, -7.9597e-01,  1.4722e-01, -1.2031e-02,\n",
      "          1.0767e+00, -1.8597e-01, -1.4262e-01,  5.3659e-01,  1.6064e+00,\n",
      "          1.9219e-01,  1.1824e+00, -2.2914e-01, -1.4954e-01,  3.3172e-01,\n",
      "          5.4246e-01, -6.6291e-03, -6.6500e-01,  1.1255e+00,  2.2724e-01,\n",
      "         -2.9655e-02,  5.9507e-01, -5.2041e-02, -1.0659e-01,  6.1314e-01,\n",
      "         -8.8548e-01, -1.2991e+00, -3.9145e-01,  4.3252e-01, -1.3237e-01,\n",
      "          2.1679e-01,  1.1232e+00,  7.1782e-01,  3.1256e-01, -8.1020e-01,\n",
      "         -2.6036e-01, -3.5692e-03]], grad_fn=<IndexBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(CLIP(**processed_full)['text_model_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 49408])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7w/g1jvvzcs7vl1y0xxcp259jjr0000gn/T/ipykernel_80827/3572423876.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label_tensor = torch.tensor(label_array)\n"
     ]
    }
   ],
   "source": [
    "def turn_to_one_hot(label_array, vocab_size):\n",
    "\n",
    "    # Your existing one-hot encoding code\n",
    "    label_tensor = torch.tensor(label_array)\n",
    "    one_hot = torch.nn.functional.one_hot(label_tensor, num_classes=vocab_size)\n",
    "\n",
    "    # # Create start token\n",
    "    # start_token = torch.zeros(13)\n",
    "    # start_token[10] = 1\n",
    "\n",
    "    # Add start token to sequence\n",
    "    \n",
    "    return one_hot\n",
    "print(turn_to_one_hot(tokenized_text['input_ids'], vocab_size).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224]) torch.Size([1, 7, 512])\n",
      "<class 'torch.Tensor'> tokenized_text['input_ids'].shape\n",
      "torch.Size([1, 7]) caption.shape\n",
      "torch.Size([1, 7]) x.shape pre embedding\n",
      "torch.Size([1, 7, 512]) x.shape\n",
      "torch.Size([1, 7, 512]) attn.shape\n",
      "torch.Size([1, 7, 512]) embedding.shape\n",
      "torch.Size([1, 7, 512]) cross attn.shape\n",
      "torch.Size([1, 7, 512]) attn.shape\n",
      "torch.Size([1, 7, 512]) embedding.shape\n",
      "torch.Size([1, 7, 512]) cross attn.shape\n",
      "torch.Size([1, 7, 512]) attn.shape\n",
      "torch.Size([1, 7, 512]) embedding.shape\n",
      "torch.Size([1, 7, 512]) cross attn.shape\n",
      "torch.Size([1, 7, 512]) attn.shape\n",
      "torch.Size([1, 7, 512]) embedding.shape\n",
      "torch.Size([1, 7, 512]) cross attn.shape\n",
      "torch.Size([1, 7, 512]) attn.shape\n",
      "torch.Size([1, 7, 512]) embedding.shape\n",
      "torch.Size([1, 7, 512]) cross attn.shape\n",
      "torch.Size([1, 7, 512]) attn.shape\n",
      "torch.Size([1, 7, 512]) embedding.shape\n",
      "torch.Size([1, 7, 512]) cross attn.shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2604,  0.0381, -0.8481,  ..., -0.0376, -0.4862, -0.0947],\n",
       "         [ 0.2296,  0.1469, -0.6619,  ...,  0.0222, -0.5660, -0.0420],\n",
       "         [ 0.2029,  0.3369, -0.5353,  ...,  0.0363, -0.5809, -0.0202],\n",
       "         ...,\n",
       "         [ 0.1553,  0.4184, -0.5808,  ...,  0.1228, -0.7123,  0.0349],\n",
       "         [ 0.0856,  0.4178, -0.6168,  ...,  0.1669, -0.6255,  0.0569],\n",
       "         [ 0.1304,  0.5361, -0.3280,  ...,  0.1989, -0.5903,  0.0642]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "transformer.eval()\n",
    "print(image_processed['pixel_values'].shape, last_hidden_state_text.shape)\n",
    "# textonehot = turn_to_one_hot(tokenized_text['input_ids'], vocab_size)\n",
    "# token_ids['pixel_values']\n",
    "print(type(tokenized_text['input_ids']), \"tokenized_text['input_ids'].shape\")\n",
    "transformer.forward(image_processed['pixel_values'], tokenized_text['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[49406,  3306,   328,   687,  1929, 49407, 49407, 49407, 49407, 49407]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "text = \"hello i am dog\"\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", max_length=10, truncation=True)\n",
    "print(tokenized_text['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11]) padding_mask.shape\n",
      "torch.Size([1, 11, 11]) padding_mask_self.shape\n",
      "(tensor([[[ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "          False],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "          False],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "          False],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "          False],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "          False],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "          False],\n",
      "         [False, False, False, False, False, False, False, False, False, False,\n",
      "          False],\n",
      "         [False, False, False, False, False, False, False, False, False, False,\n",
      "          False],\n",
      "         [False, False, False, False, False, False, False, False, False, False,\n",
      "          False],\n",
      "         [False, False, False, False, False, False, False, False, False, False,\n",
      "          False],\n",
      "         [False, False, False, False, False, False, False, False, False, False,\n",
      "          False]]]), tensor([[ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "         False]]))\n"
     ]
    }
   ],
   "source": [
    "def generate_padding_mask(caption, pad_token):\n",
    "        \"\"\"\n",
    "        Generate combined padding and causal mask for decoder self-attention.\n",
    "        Args:\n",
    "            caption: Input caption tensor of shape (batch_size, seq_len, vocab_size)\n",
    "        Returns:\n",
    "            Attention mask of shape (batch_size, seq_len, seq_len) where:\n",
    "            - pad tokens are masked with 0\n",
    "            - future tokens are masked with 0 (causal masking)\n",
    "            - valid tokens are marked with 1\n",
    "        \"\"\"\n",
    "        # batch_size, seq_length, _ = caption.shape\n",
    "        \n",
    "        # Get padding mask by checking if the last index (pad token) is 1\n",
    "        # print(caption.shape, \"caption\")\n",
    "        padding_mask = (caption.squeeze(1) != pad_token).bool()  # [batch_size, seq_len]\n",
    "        padding_mask = torch.cat([torch.ones(padding_mask.shape[0], 1, device=padding_mask.device, dtype=torch.bool), padding_mask], dim=1)\n",
    "        # Each item in the batch gets its own mask because:\n",
    "        # 1. padding_mask is [batch_size, seq_len]\n",
    "        # 2. When we do the unsqueeze operations, we maintain the batch dimension:\n",
    "        print(padding_mask.shape, \"padding_mask.shape\")\n",
    "        padding_mask_self = padding_mask.unsqueeze(1) * padding_mask.unsqueeze(2)\n",
    "        print(padding_mask_self.shape, \"padding_mask_self.shape\")\n",
    "        # Create final mask by combining padding and causal masks\n",
    "        final_mask = padding_mask_self\n",
    "        # cross_attn_mask = padding ._ma .sk\n",
    "        # print(padding_mask[0], \"padding mask\")\n",
    "        # print(cross_attn_mask[1], \"final_mask\")\n",
    "        # Create final mask by combining padding and causal masks\n",
    "        \n",
    "        return final_mask, padding_mask\n",
    "\n",
    "print(generate_padding_mask(tokenized_text['input_ids'], tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49407\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|startoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>',\n",
       " 'pad_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49407"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[49406,  3306,   328,   687,  1929, 49407, 49408, 49408, 49408, 49408]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_caption = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", max_length=25, truncation=True)\n",
    "input_ids = tokenized_text['input_ids']\n",
    "\n",
    "# Find where the EOS token (49407) is\n",
    "eos_positions = (input_ids == tokenizer.eos_token_id).nonzero()\n",
    "if len(eos_positions) > 0:\n",
    "    first_eos_pos = eos_positions[0][1]\n",
    "    \n",
    "    # Replace all padding tokens after the first EOS with 49408 (<<<PAD>>>)\n",
    "    # Keep one EOS token (49407) at the first EOS position\n",
    "    input_ids[0, first_eos_pos+1:] = 49408\n",
    "\n",
    "tokenized_text['input_ids'] = input_ids\n",
    "print(tokenized_text['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49408"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()  # Returns dict of {token: index\n",
    "vocab_size = tokenizer.vocab_size\n",
    "reverse_vocab = {idx: token for token, idx in vocab.items()}\n",
    "\n",
    "vocab[\"<<<PAD>>>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<<<PAD>>>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_vocab[49408]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtensor\u001b[49m([[[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m],\n\u001b[1;32m      2\u001b[0m          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m],\n\u001b[1;32m      3\u001b[0m          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0837\u001b[39m],\n\u001b[1;32m      4\u001b[0m          \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\n\u001b[1;32m      5\u001b[0m          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0562\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0733\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.1418\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.8268\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.8782\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.8953\u001b[39m],\n\u001b[1;32m      6\u001b[0m          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0904\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.1075\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.2103\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.9809\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0152\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0152\u001b[39m],\n\u001b[1;32m      7\u001b[0m          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.2103\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.2103\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.3130\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0665\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0494\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0494\u001b[39m]],\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m         [[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m],\n\u001b[1;32m     10\u001b[0m          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m],\n\u001b[1;32m     11\u001b[0m          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0007\u001b[39m],\n\u001b[1;32m     12\u001b[0m          \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\n\u001b[1;32m     13\u001b[0m          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0903\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0378\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0553\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7381\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7906\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.8081\u001b[39m],\n\u001b[1;32m     14\u001b[0m          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.1078\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0728\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.1253\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.8957\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.9307\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.9307\u001b[39m],\n\u001b[1;32m     15\u001b[0m          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.2129\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.1954\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.2479\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.9832\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.9657\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.9657\u001b[39m]],\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m         [[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m],\n\u001b[1;32m     18\u001b[0m          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m],\n\u001b[1;32m     19\u001b[0m          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m],\n\u001b[1;32m     20\u001b[0m          \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\n\u001b[1;32m     21\u001b[0m          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.6890\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.6890\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.7761\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.5430\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.5953\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.6127\u001b[39m],\n\u001b[1;32m     22\u001b[0m          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.7413\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.7238\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.8284\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.6999\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7347\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7347\u001b[39m],\n\u001b[1;32m     23\u001b[0m          [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.8633\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.8458\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.9330\u001b[39m,  \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7870\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.7696\u001b[39m]]])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensor' is not defined"
     ]
    }
   ],
   "source": [
    "tensor([[[ -8.1673,  -5.8411, -10.3520,  ..., -10.7300,   0.4442,  -0.2268]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor1 = torch.tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False, False, False, False, False, False,\n",
    "         False, False, False, False, False]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 false_index\n",
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False]]) expanded_mask.shape\n",
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False]]) expanded_mask.shape\n"
     ]
    }
   ],
   "source": [
    "# Create 1,8,45,45 weight tensor\n",
    "weight_tensor = torch.ones((1, 8, 45, 45))\n",
    "\n",
    "# Expand tensor1 to 45,45\n",
    "expanded_tensor1 = tensor1.expand(45, 45)\n",
    "# print(expanded_tensor1, \"expanded_tensor1.shape\")\n",
    "\n",
    "# Set last 2 rows to False\n",
    "false_index = (tensor1.squeeze(0) == False).nonzero()[0].squeeze(0).item()\n",
    "print(false_index, \"false_index\")\n",
    "expanded_mask = torch.clone(tensor1)\n",
    "print(expanded_mask, \"expanded_mask.shape\")\n",
    "expanded_mask[false_index:, :] = False\n",
    "print(expanded_mask, \"expanded_mask.shape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tensor1.squeeze(0) == False).nonzero()[0].squeeze(0).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
